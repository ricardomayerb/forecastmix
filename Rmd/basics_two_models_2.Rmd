---
title: 'The basics: steps with one specification'
author: "Ricardo Mayer"
date: "12/6/2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r source-and-lib, message=FALSE, warning=FALSE}
source('./R/combinations_functions.R')
```



We will start with a ready to use data set with quarterly, stationary series than can be used in an ordinary VAR, and we will not explain how the data munging is done. In particular, we will *not* discuss the following important points:
    - how the data was obtained
    - how monthly data was converted to quaterly frequency
    - how monthly data was *extended* to complete its current final quarter
    - how (potentially) exogenous data was forecasted in order to make it available to produce conditional forecasts
    - how each series was transformed using seasonal and oridinary differeces to render them stationary
    
All those points are discussed in the data preparation document, see *here*

## VAR-ready data set
    
We will use domestic data from Uruguay and few series that can be considered exogenous to teh country's economic activity, namely indexes of economic activity in USA, EU, Asia, Brazil and Argentina (we can choose later whether to treat a given series as exogeous or exogenous when specifying our VARs)

First, we identify the country (Uruguay) and vintage (second vintage of data gathered in 2018) Then we load the data, put it in quarterly form and print their names 


```{r loading_data}
country <- "Uruguay"
forecast_exercise_year <- 2018
forecast_exercise_number <- 3
var_output_path <- paste0("./analysis/VAR_output//edd_exercises/", forecast_exercise_year, 
                            "_exercise_", forecast_exercise_number, "/")

all_data_for_VAR_estimation  <- readRDS(paste0(var_output_path, "VAR_data_", country, ".rds"))
# print(colnames(VAR_data_for_estimation))
```

In this case, we used a  "diff-yoy" transformation for the real GDP series (first, take seasonal differences on the quarterly series and then ordinary differences on the result) and, as consequence the predictions about real gdp coming straight out the VAR will be forecasts in this metric, too. It is up to us to transform those predicted values into, say, year-on-year proportional changes.    

## Propose, test and keep or discard specifications

Given a data set with stationary variables, obteined in the previous section, a VAR especification consists in a set of endogenous variables, exogenous variables, a maximum lag value and a restriction over coefficients. Lets begin with the following specification

  - set of endogenous variables: rgdp, imp_intermediate, rpc
  - set of exogenous variables: none  (we will add them later)
  - a value for the maximum lag: 5
  - a restriction over coefficients: none or, as we will see, setting our threshold to zero. 

```{r inital_var_specification}
subset_of_variables <- c("rgdp", "imp_intermediate", "rpc")
var_data <- all_data_for_VAR_estimation[, subset_of_variables]
var_data <- na.omit(var_data)
this_lag <- 4

var_fit <- vars::VAR(y = var_data, p = this_lag, type = "const")
```

### Proposing lag values

For searching purposes we could propose a single maximum lag value, like 5, or a manually specify set of values, as 
c(3,5,6) or use some information-based criteria or add both approaches. 
The function lags_for_var, in this package, helps us to do just that. If we only specify manual values it will simply return the same value or vector, but if we chose "info" or "add_info_based_lags = TRUE" it will compute Akaike, Bayesian, SC and FPE criteria and choose the optimal lag for each of them, eliminate repeated values and return the resultig vector or its union with a manually specified vector of lags.

```{r proposing lags}
manual_single <- lags_for_var(vec_lags = 5, max_p_for_estimation = 9, endodata = var_data)

manual_multiple <- lags_for_var(vec_lags = c(5, 7), max_p_for_estimation = 9, endodata = var_data)

info_based <- lags_for_var(vec_lags = "info", max_p_for_estimation = 9, endodata = var_data, ret_info_results = TRUE)

manual_and_info <- lags_for_var(vec_lags = c(5, 7), add_info_based_lags = TRUE, max_p_for_estimation = 9, endodata = var_data)

print(manual_single)
print(manual_multiple)
print(info_based$info_criteria)
print(manual_and_info)
```
In this case, the Akaike statistic is minimized at nine lags, the Schwartz statistic at one lag and both the prediction error and hanna-quinn statistics are minimized at 4 lags. In principle, we could try all values from 1 to 9 or even higher, but, besides the costs of extra computatons a note of caution is due: unrestricted VARs tend to increase the number of coeffcients very rapidly with the maximum value of lags and given that some our data matrices can be relative short we should be wary of lag-happy specifications. For instance, the following code will estimates all VARs using from 1 to 12 lags and report to us the degrees of freedom of the residuals for each lag choice:

```{r}
var_fit_1_to_13 <-  map(1:13, ~ vars::VAR(y = var_data, p = . , type = "const"))

df_1_to_13 <- map_dbl(var_fit_1_to_13, c("varresult", "rgdp", "df.residual"))
names(df_1_to_13) <- 1:13
print(df_1_to_13)

```

So, for instance, with one lag we have 44 degrees of freedom but only 12 if we choose nine lags and it is not even possible to fit a model with 12 lags with the amount of data we have, let alone to compute some statistic based on the residuals.

And for further ilustration, here is the same count of degrees of freedom if we use five variables instead of three


```{r avar5, echo=FALSE}
var_data_5 <- na.omit(all_data_for_VAR_estimation[, 
                        c("rgdp", "imp_intermediate", "rpc", "fbcf", "m2")])

var_fit_1_to_13_var5 <-  map(1:13, ~ vars::VAR(y = var_data_5, p = . , type = "const"))

df_1_to_13_var5 <- map_dbl(var_fit_1_to_13_var5, c("varresult", "rgdp", "df.residual"))
names(df_1_to_13_var5) <- 1:13
print(df_1_to_13_var5)
```


The is no loss of generality in choosing to work the rest of this document with just one lag choice: $p = 4$. In the next vignette we will show how to work with several specifications, including the case of several lag values for a given set of variables.

### Try some zero-restrictions on coefficients

The proliferation of parameters induced by choosig more variables or longer lags, leads to the desire of at least try more parsimonious formulations of our VARs. There are several ways to implement more parameter-frugal models, but one easy, direct one is to impose zero restrictions on parameters based on t-statistics. So far, it is the only procedure in this package that looks into more parsimonious version of a VAR. The steps are fairly simple:

  1. Fit a VAR (just as we did in the previous section)
  2. Choose a threshold value for the t-statistic, e.g. 1.65 or 2 (or zero to signal an unrestricted model)
  3. Use the restrict function from the vars package and set the options method to "ser" and thresh to the value you chose in the previous step
  4. If the threshold is set to, say, 1.65 then every coefficient whith a t-statistic lower than 1.65 will be set to zero (the model will be fit using a restriction matrix with zeros on those positions and ones elsewhere) 
  4. The output of restrict if your new VAR object and can be tested, printed and used in the same way of unrestricted ones



```{r some_tests}
# unrestrcited
var_fit_u <- vars::VAR(y = var_data, p = this_lag , type = "const")

# restricted: all coeficients from var_fit_u with t-statictics less than 1.65 are set to zero
var_fit_r_165 <- restrict(var_fit_u, method = "ser", thresh = 1.65)

# same as above but threshold is 2
var_fit_r_200 <- restrict(var_fit_u, method = "ser", thresh = 2)

# a treshold of zero should render the unrestricted model
var_fit_r_000 <- restrict(var_fit_u, method = "ser", thresh = 0)

print(summary(var_fit_r_200$varresult$rgdp))
```

Looking at the outupt of model var_fit_r_200 we can see a considerable reduction in the number of estimated parameters which reflects itself in the larger number of degrees of freedom of the residuals (41 in rgdp equation and, not printed, 42 for the other two equations), compared against 32 in the unrestricted case. Notice that it may even be the case that one (or more) of the equations drops out entirely if no coefficients survive this restriction. If, for instance, that happens to the rpc equation then it will be no longe a 3-variables VAR and it will not be added to the list of 3-variables specifications for what comes next.


### Check model stability

The next step consists in checkig wheter the estimated VAR model is stable, i.e. if 
all roots of its characteristic polynomial fall inside the unitary circle.

```{r check_stability}
roots_u <- vars::roots(var_fit_u)
print(roots_u)
is_stable <- all(roots_u < 1)
print(is_stable)
```
Only stable models are kept. Sometimes large coefficient estimates but with very low precision cause instability, which is another reason to explore restricted variants whith fewer but more significant coefficients.

### Test whether residuals are white noise

Surviving models are subject to a portmanteau test of their residuals, discarding models where the null of white noise of the residuals is rejected

```{r whitenoise}
residuals_white_noise <- check_resid_VAR(var_fit_u)
print(residuals_white_noise)
```

So in this case we cannot reject the null hipotesis of white-noiseness of the residuals.
Behid the scenes the function checkresidVAR uses vars::serial.test to carry on the test and then translate the output into a logical statement based on the reported p-value:

```{r serial_test}
stest_output <- serial.test(var_fit_u)
stest_output
```

Default arguments of check_resid_VAR are set to match those of serial.test. One option that could be worth exploring is the type of test: the default test is based on the asymptotic distribution of the statistics, but we can try another statistic that tries to adjust by sample size by using type = "PT.adjusted", which will be passed down to serial.test  and see if it reaches the same conclusion of the asymptotic test:



```{r serial_test_adj}
# this can be done by using 
stest_output_adj <- serial.test(var_fit_u, type = "PT.adjusted")
print(stest_output_adj)

# so check_resid_VAR would still return TRUE
residuals_white_noise_adj <- check_resid_VAR(var_fit_u, type = "PT.adjusted")
print(residuals_white_noise_adj)

```

### Specification is ready for forecasting asessesment

So far our unrestricted VAR(4) with three variables is stable, with white noise residuals. Only after that is verified we move onto forecasting and forecast performance evaluation, otherwise we pick the next speficication on our list and repeat the checking process.

### Can we put all this in a tibble?

Sure we do. The idea is to form a long tibble with all candidate specifications. In concrete each row represent a specification: variables, lag and threshold. Then create columns that contains fitted models, stability checks, residual tests and, as we will see shortly, forecasts and forecast evaluation measures.

```{r onemodeltibble_1}

current_variables <- c("rgdp", "rpc", "imp_intermediate")
current_lag <- 4
current_thresh <- 0

# Basic infotmation for specification: variables, lag and restriction
one_specification_basics <- tibble(variables = list(current_variables),
                            lag = current_lag,
                            t_threshold = current_thresh)

# Our function fit_VAR_rest calls both vars::VAR and vars::restrict
# After fitting the model we apply stability and residual tests
one_specification <- one_specification_basics %>% 
  mutate(fit =  pmap(list(variables, lag, t_threshold),
                     ~ fit_VAR_rest(all_data_for_VAR_estimation, 
                                    variables = ..1, p = ..2, t_thresh = ..3)),
         is_stable = map_lgl(fit, ~ all(vars::roots(.x) < 1)),
         is_white_noise = map_lgl(fit, ~ check_resid_VAR(.x))
         )

one_specification
```

The function fit_VAR_rest basically calls vars::VAR and vars::restrict, but also handles what to do when estimation fails and it generates the exogenous variables matrix if needed. 

Accesing the elements inside this tibble is straightforward

```{r accesstibble}
# traditional way is one_specification$variables[[1]]),
# but I prefer purrr's pluck syntax, it seems clearer to me.
pluck(one_specification, "variables", 1)

# the integer 1 refers here to the first element of the chosen column
# i.e. to the first row of the tibble
pluck(one_specification, "lag", 1)

pluck(one_specification, "t_threshold", 1)

pluck(one_specification, "is_stable", 1)

pluck(one_specification, "is_white_noise", 1)

# this gets the all the fitted model fitted models, stored in the 
# "fit" column, then take the first of them.
first_fitted <- pluck(one_specification, "fit", 1)

# the usual print method for objects of class varest
print(first_fitted)

# top level content of a varest object
names(pluck(one_specification, "fit", 1))

pluck(one_specification, "fit", 1, "obs")


# elements inside varresult
names(pluck(one_specification, "fit", 1, "varresult"))
names(pluck(one_specification, "fit", 1, "varresult", "rgdp"))


# A longer instruction: from one_specification, first take column "fit", 
# then take the first element of that column, then go inside "varresult", 
# then to "rgdp" and finally get the fitted.values object
pluck(one_specification, "fit", 1, "varresult", "rgdp", "coefficients")

# the traditional way is to write:
# one_specification[["fit"]][[1]][["varresult"]][["rgdp"]][["coefficients"]]
# or maybe, since its common to access columns with $, to write this
# one_specification$fit[[1]][["varresult"]][["rgdp"]][["coefficients"]]

```





## Forecast evaluation: time series cross validation

### Out of sample forecasts

To produce an ordiary forecast using a fitted VAR, you can use vars::predict function, but we have written the function 
forecast_VAR_one_row specifically with the previous tibble structure in mind and to handle the case of exogenous regressors transparently. Both function return an object of class varest, which in this case will contain the same output from a fitted model plus the mean forecasts and estimates of some percentiles for each equation (e.g. for rgdp, rpc and imp_intermediate)

```{r forecastfullsample}

# number of periods to forecast, in this case it is 8 quarters
fc_horizon <- 8

# add a new column containig the forecast objects
one_specification <- one_specification %>% 
    mutate(fc_object_raw = map2(fit, variables,
                                ~ forecast_VAR_one_row(
                                  fit = .x, variables = .y, h = fc_horizon
                                  )
                                )
           )

# this would return the entire forecast object of our first model
fc_raw_1 <- pluck(one_specification, "fc_object_raw", 1) 

# a time series with the mean forecast from the rgdp equation 
print(pluck(fc_raw_1, "forecast", "rgdp", "mean"))

```

### Changing the transformation of a series

In this case the rgdp data for the country was transformed to a "diff-yoy" format (a seasonal difference first and then ordinary diferences) to render it statioary and suitable for the VAR. So the fitted values and forecast also are diff-yoy values of rgdp. In finite samples and only approximate models is not true that the best models predicting a diff-yoy of rgdp are also the best models predicting yoy rgdp (which is the measure we ultimately want to present), so our ranking of models must be explicitely done in terms of errors about yoy rgdp. This demands that we transform our forecasted diff-yoy values and conver them into forecasted yoy values. In the next section we will use the later to form relevant forecast errors

```{r importrgdplevel}
excel_data_path <- paste0("./data/edd_exercises/", forecast_exercise_year, 
                            "_exercise_", forecast_exercise_number, "/")
country_data_ts <- get_raw_data_ts(country = country, data_path = excel_data_path)
external_data_ts <- get_raw_external_data_ts(data_path = excel_data_path)
data_ts <- ts.union(country_data_ts, external_data_ts)
colnames(data_ts) <- c(colnames(country_data_ts), colnames(external_data_ts))
rgdp_level_ts <- data_ts[, "rgdp"]
rgdp_level_ts <- na.omit(rgdp_level_ts)

```

now the conversion

```{r fcrgdpdiffyoytoyoy}
fc_rgdp_raw_1 <- pluck(one_specification, "fc_object_raw", 1,
                       "forecast", "rgdp", "mean") 

fc_rgdp_yoy <- any_fc_2_fc_yoy(current_fc = fc_rgdp_raw_1, 
                               rgdp_transformation = "diff_yoy", 
                               rgdp_level_ts = rgdp_level_ts) 

print(fc_rgdp_raw_1)
print(fc_rgdp_yoy)

```


### Training-sample forecast: time series cross validation
As we advanced above, we want to measure the errors in forecasting YoY growth of rgdp. Remember that for this country rgdp realized data is in diff-yoy form, just as its forecasted values. The function cv_var_from_one_row takes our model contained in a tibble's row and does a time-series cross validation of whatever series we flag as our "target variable". It returns objects with forecasted values, realized values and their differences (i.e. forecasting errors). However, unless the target variable was itself in yoy form, then those errors are not the ones we sought. In the case of Uruguay we will take the eight rgdp series of realizations, the eight forecasted series, transform each one to YoY and compute new, yoy, errors. Once we have yoy-forecasting error 

```{r cvonerow}

training_length <- 25
n_cv <- 8


one_specification <-  one_specification %>%
    mutate(cv_obj = pmap(list(fit, variables, lag, t_threshold),
                         ~ cv_var_from_one_row(var_data = var_data, fit = ..1,
                                               variables = ..2, lags = ..3,
                                               this_thresh = ..4,
                                               h = fc_horizon, n_cv = n_cv,
                                               training_length = training_length
                                               )
                         )
           )


diff_yoy_test_data <- pluck(one_specification, "cv_obj", 1, "cv_test_data") 
diff_yoy_fcs <- pluck(one_specification, "cv_obj", 1, "cv_fcs") 
diff_yoy_errors <- map2(diff_yoy_test_data, diff_yoy_fcs, ~ .x - .y) 

```




### Forecast errors and accuracy measures

s


