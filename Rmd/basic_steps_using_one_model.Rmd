---
title: 'Model specification and accuracy measures with one model, step by step.'
author: "Ricardo Mayer"
date: "12/6/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r source-and-lib, message=FALSE, warning=FALSE}
source('./R/combinations_functions.R')
```

We will start with a ready to use data set with quarterly, stationary series than can be used in an ordinary VAR, and we will not explain how the data munging is done. In particular, we will *not* discuss the following important points:
    - how the data was obtained
    - how monthly data was converted to quaterly frequency
    - how monthly data was *extended* to complete its current final quarter
    - how (potentially) exogenous data was forecasted in order to make it available to produce conditional forecasts
    - how each series was transformed using seasonal and oridinary differeces to render them stationary
    
All those points are discussed in the data preparation document, see *here*

## VAR-ready data set
    
We will use domestic data from Uruguay and few series that can be considered exogenous to teh country's economic activity, namely indexes of economic activity in USA, EU, Asia, Brazil and Argentina (we can choose later whether to treat a given series as exogeous or exogenous when specifying our VARs)

First, we identify the country (Uruguay) and vintage (second vintage of data gathered in 2018) Then we load the data, put it in quarterly form and print their names 


```{r loading_data}
country <- "Uruguay"
forecast_exercise_year <- 2018
forecast_exercise_number <- 3
var_output_path <- paste0("./analysis/VAR_output//edd_exercises/", forecast_exercise_year, 
                            "_exercise_", forecast_exercise_number, "/")

all_data_for_VAR_estimation  <- readRDS(paste0(var_output_path, "VAR_data_", country, ".rds"))
# print(colnames(VAR_data_for_estimation))



```

In this case, we used a  "diff-yoy" transformation for the real GDP series (first, take seasonal differences on the quarterly series and then ordinary differences on the result) and, as consequence the predictions about real gdp coming straight out the VAR will be forecasts in this metric, too. It is up to us to transform those predicted values into, say, year-on-year proportional changes.    

## Propose, test and keep or discard specifications

Given a data set with stationary variables, obteined in the previous section, a VAR especification consists in a set of endogenous variables, exogenous variables, a maximum lag value and a restriction over coefficients. Lets begin with the following specification

  - set of endogenous variables: rgdp, imp_intermediate, rpc
  - set of exogenous variables: none  (we will add them later)
  - a value for the maximum lag: 5
  - a restriction over coefficients: none or, as we will see, setting our threshold to zero. 

```{r inital_var_specification}
subset_of_variables <- c("rgdp", "imp_intermediate", "rpc")
var_data <- all_data_for_VAR_estimation[, subset_of_variables]
var_data <- na.omit(var_data)
this_lag <- 4

var_fit <- vars::VAR(y = var_data, p = this_lag, type = "const")
```

### Proposing lag values

For searching purposes we could propose a single maximum lag value, like 5, or a manually specify set of values, as 
c(3,5,6) or use some information-based criteria or add both approaches. 
The function lags_for_var, in this package, helps us to do just that. If we only specify manual values it will simply return the same value or vector, but if we chose "info" or "add_info_based_lags = TRUE" it will compute Akaike, Bayesian, SC and FPE criteria and choose the optimal lag for each of them, eliminate repeated values and return the resultig vector or its union with a manually specified vector of lags.

```{r proposing lags}
manual_single <- lags_for_var(vec_lags = 5, max_p_for_estimation = 9, endodata = var_data)

manual_multiple <- lags_for_var(vec_lags = c(5, 7), max_p_for_estimation = 9, endodata = var_data)

info_based <- lags_for_var(vec_lags = "info", max_p_for_estimation = 9, endodata = var_data, ret_info_results = TRUE)

manual_and_info <- lags_for_var(vec_lags = c(5, 7), add_info_based_lags = TRUE, max_p_for_estimation = 9, endodata = var_data)

print(manual_single)
print(manual_multiple)
print(info_based$info_criteria)
print(manual_and_info)
```
In this case, the Akaike statistic is minimized at nine lags, the Schwartz statistic at one lag and both the prediction error and hanna-quinn statistics are minimized at 4 lags. In principle, we could try all values from 1 to 9 or even higher, but, besides the costs of extra computatons a note of caution is due: unrestricted VARs tend to increase the number of coeffcients very rapidly with the maximum value of lags and given that some our data matrices can be relative short we should be wary of lag-happy specifications. For instance, the following code will estimates all VARs using from 1 to 12 lags and report to us the degrees of freedom of the residuals for each lag choice:

```{r}
var_fit_1_to_13 <-  map(1:13, ~ vars::VAR(y = var_data, p = . , type = "const"))

df_1_to_13 <- map_dbl(var_fit_1_to_13, c("varresult", "rgdp", "df.residual"))
names(df_1_to_13) <- 1:13
print(df_1_to_13)

```

So, for instance, with one lag we have 44 degrees of freedom but only 12 if we choose nine lags and it is not even possible to fit a model with 12 lags with the amount of data we have, let alone to compute some statistic based on the residuals.

And for further ilustration, here is the same count of degrees of freedom if we use five variables instead of three


```{r avar5, echo=FALSE}
var_data_5 <- na.omit(all_data_for_VAR_estimation[, 
                        c("rgdp", "imp_intermediate", "rpc", "fbcf", "m2")])

var_fit_1_to_13_var5 <-  map(1:13, ~ vars::VAR(y = var_data_5, p = . , type = "const"))

df_1_to_13_var5 <- map_dbl(var_fit_1_to_13_var5, c("varresult", "rgdp", "df.residual"))
names(df_1_to_13_var5) <- 1:13
print(df_1_to_13_var5)
```


The is no loss of generality in choosing to work the rest of this document with just one lag choice: $p = 4$. In the next vignette we will show how to work with several specifications, including the case of several lag values for a given set of variables.

### Try some zero-restrictions on coefficients

The proliferation of parameters induced by choosig more variables or longer lags, leads to the desire of at least try more parsimonious formulations of our VARs. There are several ways to implement more parameter-frugal models, but one easy, direct one is to impose zero restrictions on parameters based on t-statistics. So far, it is the only procedure in this package that looks into more parsimonious version of a VAR. The steps are fairly simple:

  1. Fit a VAR (just as we did in the previous section)
  2. Choose a threshold value for the t-statistic, e.g. 1.65 or 2 (or zero to signal an unrestricted model)
  3. Use the restrict function from the vars package and set the options method to "ser" and thresh to the value you chose in the previous step
  4. If the threshold is set to, say, 1.65 then every coefficient whith a t-statistic lower than 1.65 will be set to zero (the model will be fit using a restriction matrix with zeros on those positions and ones elsewhere) 
  4. The output of restrict if your new VAR object and can be tested, printed and used in the same way of unrestricted ones



```{r some_tests}
# unrestrcited
var_fit_u <- vars::VAR(y = var_data, p = this_lag , type = "const")

# restricted: all coeficients from var_fit_u with t-statictics less than 1.65 are set to zero
var_fit_r_165 <- restrict(var_fit_u, method = "ser", thresh = 1.65)

# same as above but threshold is 2
var_fit_r_200 <- restrict(var_fit_u, method = "ser", thresh = 2)

# a treshold of zero should render the unrestricted model
var_fit_r_000 <- restrict(var_fit_u, method = "ser", thresh = 0)

print(summary(var_fit_r_200$varresult$rgdp))
```

Looking at the outupt of model var_fit_r_200 we can see a considerable reduction in the number of estimated parameters which reflects itself in the larger number of degrees of freedom of the residuals (41 in rgdp equation and, not printed, 42 for the other two equations), compared against 32 in the unrestricted case. Notice that it may even be the case that one (or more) of the equations drops out entirely if no coefficients survive this restriction. If, for instance, that happens to the rpc equation then it will be no longe a 3-variables VAR and it will not be added to the list of 3-variables specifications for what comes next.


### Check model stability

The next step consists in checkig wheter the estimated VAR model is stable, i.e. if 
all roots of its characteristic polynomial fall inside the unitary circle.

```{r check_stability}
roots_u <- vars::roots(var_fit_u)
print(roots_u)
is_stable <- all(roots_u < 1)
print(is_stable)
```
Only stable models are kept. Sometimes large coefficient estimates but with very low precision cause instability, which is another reason to explore restricted variants whith fewer but more significant coefficients.

### Test whether residuals are white noise

Surviving models are subject to a portmanteau test of their residuals, discarding models where the null of white noise of the residuals is rejected

```{r whitenoise}
residuals_white_noise <- check_resid_VAR(var_fit_u)
print(residuals_white_noise)
```

So in this case we cannot reject the null hipotesis of white-noiseness of the residuals.
Behid the scenes the function checkresidVAR uses vars::serial.test to carry on the test and then translate the output into a logical statement based on the reported p-value:

```{r serial_test}
stest_output <- serial.test(var_fit_u)
stest_output
```

Default arguments of check_resid_VAR are set to match those of serial.test. One option that could be worth exploring is the type of test: the default test is based on the asymptotic distribution of the statistics, but we can try another statistic that tries to adjust by sample size by using type = "PT.adjusted", which will be passed down to serial.test  and see if it reaches the same conclusion of the asymptotic test:



```{r serial_test_adj}
# this can be done by using 
stest_output_adj <- serial.test(var_fit_u, type = "PT.adjusted")
print(stest_output_adj)

# so check_resid_VAR would still return TRUE
residuals_white_noise_adj <- check_resid_VAR(var_fit_u, type = "PT.adjusted")
print(residuals_white_noise_adj)

```

### Specification is ready for forecasting asessesment

So far our unrestricted VAR(4) with three variables is stable, with white noise residuals. Only after that is verified we move onto forecasting and forecast performance evaluation, otherwise we pick the next speficication on our list and repeat the checking process.

### Can we put all this in a tibble?

Sure we do. The idea is to form a long tibble with all candidate specifications. In concrete each row represent a specification: variables, lag and threshold. Then create columns that contains fitted models, stability checks, residual tests and, as we will see shortly, forecasts and forecast evaluation measures.

```{r onemodeltibble_1}

current_variables <- c("rgdp", "rpc", "imp_intermediate")
current_lag <- 4
current_thresh <- 0

# Basic infotmation for specification: variables, lag and restriction
one_specification_basics <- tibble(variables = list(current_variables),
                            lag = current_lag,
                            t_threshold = current_thresh)

# Our function fit_VAR_rest calls both vars::VAR and vars::restrict
# After fitting the model we apply stability and residual tests
one_specification <- one_specification_basics %>% 
  mutate(fit =  pmap(list(variables, lag, t_threshold),
                     ~ fit_VAR_rest(all_data_for_VAR_estimation, 
                                    variables = ..1, p = ..2, t_thresh = ..3)),
         is_stable = map_lgl(fit, ~ all(vars::roots(.x) < 1)),
         is_white_noise = map_lgl(fit, ~ check_resid_VAR(.x))
         )

one_specification
```

The function fit_VAR_rest basically calls vars::VAR and vars::restrict, but also handles what to do when estimation fails and it generates the exogenous variables matrix if needed. 

Accesing the elements inside this tibble is straightforward

```{r accesstibble}
# traditional way is one_specification$variables[[1]]),
# but I prefer purrr's pluck syntax, it seems clearer to me.
pluck(one_specification, "variables", 1)

# the integer 1 refers here to the first element of the chosen column
# i.e. to the first row of the tibble
pluck(one_specification, "lag", 1)

pluck(one_specification, "t_threshold", 1)

pluck(one_specification, "is_stable", 1)

pluck(one_specification, "is_white_noise", 1)

# this gets the all the fitted model fitted models, stored in the 
# "fit" column, then take the first of them.
first_fitted <- pluck(one_specification, "fit", 1)

# top level content of a varest object
print(names(first_fitted))

pluck(one_specification, "fit", 1, "obs")

# elements inside varresult
names(pluck(one_specification, "fit", 1, "varresult"))
names(pluck(one_specification, "fit", 1, "varresult", "rgdp"))


# A longer instruction: from one_specification, first take column "fit", 
# then take the first element of that column, then go inside "varresult", 
# then to "rgdp" and finally get the fitted.values object
pluck(one_specification, "fit", 1, "varresult", "rgdp", "fitted.values")

# the traditional way is to write:
# one_specification[["fit"]][[1]][["varresult"]][["rgdp"]][["fitted.values"]]
# or maybe, since its common to access columns with $, to write this
# one_specification$fit[[1]][["varresult"]][["rgdp"]][["fitted.values"]]

```


## Forecast evaluation: time series cross validation

### Out of sample forecasts

To produce an ordiary forecast using a fitted VAR, you can use vars::predict function, but we have written the function 
forecast_VAR_one_row specifically with the previous tibble structure in mind and to handle the case of exogenous regressors transparently. Both function return an object of class varest, which in this case will contain the same output from a fitted model plus the mean forecasts and estimates of some percentiles for each equation (e.g. for rgdp, rpc and imp_intermediate)

```{r forecastfullsample}

# number of periods to forecast, in this case it is 8 quarters
fc_horizon <- 8

# add a new column containig the forecast objects
one_specification <- one_specification %>% 
    mutate(fc_object_raw = map2(fit, variables,
                                ~ forecast_VAR_one_row(
                                  fit = .x, variables = .y, h = fc_horizon
                                  )
                                )
           )

# this would return the entire forecast object of our first model
fc_raw_1 <- pluck(one_specification, "fc_object_raw", 1) 

# a time series with the mean forecast from the rgdp equation 
print(pluck(fc_raw_1, "forecast", "rgdp", "mean"))

```




### Training-sample forecast: time series cross validation

In this case we have data until 2018Q2, thus if we estimate the model using data until 2016Q2 we could make forecasts about rgdp in 2016Q3, 2016Q4, etc. until 2018Q2 and we could contrast such predictions against actual realizations. That's how we obtain prediction error. Concretely such an exercise would provide one 1-step-ahead forecast error, one 2-steps-ahead forecast error, etc. up to one 8-steps-ahead forecast error. But what if we want a second value of, say, a 8-steps-ahead forecast error? well, we could estimate again our model with data up to 2016, forecast 2016Q2, 2016Q3 up to 2018Q1 and that last value is an 8-steps ahead prediction, and we do have actual data for 2018Q1 and there fore we have got a second 8-steps-ahead forecast error, but we have also got a second 1-step-ahead error with the prediction aboput 2016Q2 and a second 2-steps-ahead forecast error using this last forecasted value of 2016Q3, etc. It easy to see how estimating the model a third time with data up 2017Q4 will provide us a third batch of 1,2,3 ... and 8 steps-ahead forecasting error. We repeat this backward process until we have as many forecast error of each forecast horizon as we want or are possible given our data. Each one of those estimate-forecast-contrast over different subsamples of data is call a cross-validation run.

In the example below the parameter n_cv controls the number of cross validation runs and we set n_cv equal to 10, so we should end up with ten 1-step-ahead errors, ten 2-steps-ahead errors and so on. Let's compute those ten cv runs and print the forecasts made in each cv round.


```{r cvonerow}
training_length <- 28 # seven years of data
n_cv <- 10


one_specification_28 <-  one_specification %>%
    mutate(cv_obj = pmap(list(fit, variables, lag, t_threshold),
                         ~ cv_var_from_one_row(var_data = all_data_for_VAR_estimation,
                                               fit = ..1,
                                               variables = ..2, lags = ..3,
                                               this_thresh = ..4,
                                               h = fc_horizon, n_cv = n_cv,
                                               training_length = training_length
                                               )
                         )
           )


diff_yoy_test_data_28 <- pluck(one_specification_28, "cv_obj", 1, "cv_test_data") 
diff_yoy_fcs_28 <- pluck(one_specification_28, "cv_obj", 1, "cv_fcs") 
diff_yoy_errors_28 <- map2(diff_yoy_test_data_28, diff_yoy_fcs_28, ~ .x - .y) 

print(diff_yoy_fcs_28[[1]])
print(diff_yoy_fcs_28[[2]])
print(diff_yoy_fcs_28[[10]])
```


As expected, since the forecast horizon is eight quarters, each cv-rounds has eight forecast of rgdp. The first set is the more "recent" one, starting at 2016Q3 and the last one starts forecasting at 2014Q2.

The portion of data used to asess forecast accuracy is called test data, and the portion used to estimate the model is called the training data. We already know where the first training dataset ends, 2016Q2, but where it starts?. One answer could be 2006Q2, the beggining of the original VAR data. In that case trainig sample size is 41 observations. But what about the tenth cv round? Given that training sample ends in 2014Q1, starting in 2006Q2 implies a trining sample size of 32. Do we want this heterogeneity in training sample sizes? Maybe. Perhaps we want to neutralize the effect of different sample sizes and restrict all 10 cv rounds to use, say, 28 observations in their training sets to estimate the models. The first line in the previous chunk, training_length <- 28, does precisely that. Of course you could have used  training_length <- 32 or 25 if that is what you need/want. However, the availability of data will not allow you to use training_length <- 33 because there is not enough observations in your data set. For one cv round training sample size is T-h, where T is the total number of observations usable for VAR (that is, after transforming variables, which tipically imply some loss of observations) and h is the forecast horizon, which is in the sample size we need for the test set in order to compute forecast errors. In our case, N=49 and h=8. So if we want to do just one cv round (not advisable) you could reserve 8 observations for testing and use 41 observations for estimation (49-8). If you want a second cv round, for that round your test set moves one quarter back to the origin, so instead of 41 observations you have only 40 before the test set. It's easy to see that in general the maximum training sample size for the n-th cv round is $T-h-n+1$. The tenth cv round in our case can use at most $49-8-10-1$ observations in its training sample. Notice that T depends on the particular choice of variables: since it has to be a balanced dataset, T is effectively the length of the shortest among the chosen variables.  


If you try to impose a common sample size for a given number cv rounds (or, equivalently too many cv rounds for a given length of the training sample) that will need, in at least one of the rounds,  more data than it is available, you will receive a wanring and the program will crash due to the existence of missign values in the data matrix of the VAR:

```{r cvtoolongtraining}
training_length <- 28 # seven years of data
n_cv <- 10

one_specification_33 <-  one_specification %>%
    mutate(cv_obj = pmap(list(fit, variables, lag, t_threshold),
                         ~ cv_var_from_one_row(var_data = all_data_for_VAR_estimation,
                                               fit = ..1,
                                               variables = ..2, lags = ..3,
                                               this_thresh = ..4,
                                               h = fc_horizon, n_cv = n_cv,
                                               training_length = training_length
                                               )
                         )
           )
```


The following code will impose the largest possible common sample size for all cv rounds (32 in this case) with out having to compute it youself, which is handy when you are trying many different variable choices and therefore postentially many different total observations.

```{r cvcommonmax}
training_length <- "common_max" # it should be 32 for this data and variables
n_cv <- 10

one_specification_commax <-  one_specification %>%
    mutate(cv_obj = pmap(list(fit, variables, lag, t_threshold),
                         ~ cv_var_from_one_row(var_data = all_data_for_VAR_estimation,
                                               fit = ..1,
                                               variables = ..2, lags = ..3,
                                               this_thresh = ..4,
                                               h = fc_horizon, n_cv = n_cv,
                                               training_length = training_length
                                               )
                         )
           )

```


Finally if you want to allow as many observations as possible in each cv rounds, setting a *fixed origin* in 2006Q2 for all rounds   (and therefore using 41 observations in the first round, 40 in the second ... until 32 in the tenth round) you have to set to training length to per_cv_maxs as below:




```{r cvpercvmax}
training_length <- "per_cv_maxs" # it should be 32 for this data and variables
n_cv <- 10

one_specification <-  one_specification %>%
    mutate(cv_obj = pmap(list(fit, variables, lag, t_threshold),
                         ~ cv_var_from_one_row(var_data = all_data_for_VAR_estimation,
                                               fit = ..1,
                                               variables = ..2, lags = ..3,
                                               this_thresh = ..4,
                                               h = fc_horizon, n_cv = n_cv,
                                               training_length = training_length
                                               )
                         )
           )

```




### Forecast errors: from diff_yoy to yoy

So far, we have managed to produce ten sets of forecasts and their correspondent data realization (and therefore forecast errors). But, since rgdp for Uruguay's VAR is in diff-yoy form, these are not the forecasts we are really interested in (i.e. yoy growth) and need to be transformed back into yoy form. It turns out that rankings based on diff-yoy forecast errors are not identical to ranking based on yoy forecast errors. Since we want to ultimately select models with good yoy-predicting record, we need to transform our cv-outputs to yoy form before computing any performance measure.

The plan is fairly simple: take diff-yoy forecasts and using yoy-data turn them into yoy-forecasts, then take diff-yoy test data and again using yoy-data, obtain yoy test data. After that, compute yoy forecasting error. In general, a sufficient input is level historical data, that can be turned in yoy form if needed as in this case.



In this case the rgdp data for the country was transformed to a "diff-yoy" format (a seasonal difference first and then ordinary diferences) to render it statioary and suitable for the VAR. So the fitted values and forecast also are diff-yoy values of rgdp. In finite samples and only approximate models is not true that the best models predicting a diff-yoy of rgdp are also the best models predicting yoy rgdp (which is the measure we ultimately want to present), so our ranking of models must be explicitely done in terms of errors about yoy rgdp. This demands that we transform our forecasted diff-yoy values and conver them into forecasted yoy values. In the next section we will use the later to form relevant forecast errors.

Let's transform our rgdp mean forecast from diff-yoy to yoy (first difference is a linear operation, so we can interchange it with expectations) 

```{r getrgdpyoy, include=FALSE}

excel_data_path <- paste0("./data/edd_exercises/", forecast_exercise_year, 
                            "_exercise_", forecast_exercise_number, "/")

country_data_ts <- get_raw_data_ts(country = country, data_path = excel_data_path)
external_data_ts <- get_raw_external_data_ts(data_path = excel_data_path)
data_ts <- ts.union(country_data_ts, external_data_ts)
colnames(data_ts) <- c(colnames(country_data_ts), colnames(external_data_ts))

rgdp_level <- data_ts[,"rgdp"]
rgdp_level <- na.omit(data_ts[,"rgdp"])
rgdp_yoy <- make_yoy_ts(rgdp_level)
```

We generally have access to level data of a variable and our function will create a yoy version of it and then use yoy data to obtain yoy version of forecasts

```{r diffyoytoyoy}

# first model in the tible, rgdp forecasts from all ten rounds:
fc_rgdp_raw_all_rounds <- pluck(one_specification, "cv_obj", 1, "cv_fcs")
# first model in the tible, rgdp test data from all ten rounds:
td_rgdp_raw_all_rounds <- pluck(one_specification, "cv_obj", 1, "cv_test_data")

# only from the 4th round:
fc_rgdp_raw_cv4 <- fc_rgdp_raw_all_rounds[[4]]
td_rgdp_raw_cv4 <- td_rgdp_raw_all_rounds[[4]]


fc_rgdp_yoy_cv4 <- any_fc_2_fc_yoy(fc_rgdp_raw_cv4, rgdp_transformation = "diff_yoy", rgdp_level_ts = rgdp_level)
td_rgdp_yoy_cv4 <- any_fc_2_fc_yoy(td_rgdp_raw_cv4, rgdp_transformation = "diff_yoy", rgdp_level_ts = rgdp_level)



```

Let's take a closer look, because it is important to get this critical step right. Take the rgdp series, then its yoy transformation and finally the conversion from diffyoy to yoy of test data, as way to verify that is working correctly

```{r verifydiffyoytoyoy_data}

rgdp_yoy <- make_yoy_ts(rgdp_level)

rgdp_level_subset <- window(rgdp_level, start = c(2014, 3), end = c(2017, 3))
rgdp_yoy_subset <- window(rgdp_yoy, start = c(2015, 3), end = c(2017, 3))
rgdp_diffyoy_subset <- diff(rgdp_yoy_subset)



data_verify <- ts.union(rgdp_level_subset, rgdp_yoy_subset, td_rgdp_yoy_cv4, 
               rgdp_diffyoy_subset, td_rgdp_raw_cv4)

colnames(data_verify) <- c("level", "yoy", "cv_yoy", "diff_yoy", "cv_diff_yoy")


manual_second_yoy <- (rgdp_level_subset[6] - rgdp_level_subset[2])/rgdp_level_subset[2]
manual_third_yoy <- (rgdp_level_subset[7] - rgdp_level_subset[3])/rgdp_level_subset[3]
manual_first_diffyoy <- rgdp_yoy_subset[2] - rgdp_yoy_subset[1]
manual_second_diffyoy <- rgdp_yoy_subset[3] - rgdp_yoy_subset[2]

names(manual_second_yoy) <- "manual"
names(manual_first_diffyoy) <- "manual"
names(manual_third_yoy) <- "manual"
names(manual_second_diffyoy) <- "manual"


# put all of them together for comparison
print(data_verify)


# they should be all equal
print(c(manual_second_yoy, data_verify[6, "yoy"], data_verify[6, "cv_yoy"]))

# they should be all equal
print(c(manual_third_yoy, data_verify[7, "yoy"], data_verify[7, "cv_yoy"]))

# they should be all equal
print(c(manual_first_diffyoy, data_verify[6, "diff_yoy"], data_verify[6, "cv_diff_yoy"]))

# they should be all equal
print(c(manual_second_diffyoy, data_verify[7, "diff_yoy"], data_verify[7, "cv_diff_yoy"]))

```

The data_very matrix show the initial input (level of rgdp), the intermediate input (yoy growth of rgdp), a direct computation of diff_yoy and the cv version of rgdp which is also diff_yoy and the reconstructed yoy version of test data. You can corroborate by yourself that the transformations are correct, but we provide two manual verifications for dates 2015Q4 and 2016Q1. The crucial input is the yoy growth of rgdp in 2014Q3: we use this coupled with cv_diff_yoy to infer the  values in cv_yoy. Notice that the discrepancy in starting dates between yoy and cv_yoy columns does make sense: cv_yoy is needed to obtain forecast errors and forecast do not begin in this case (4th round of cv) until 2015Q4.


Now, lets repeat the assesment for the forecasted values of rgdp. We will use the realized data of 2015Q3 yoy rgdp and forecasted values of diff-yoy rgdp since 2015Q4 to infer the corresponding yoy. For brevity we will recontruct manually only the first two yoy forecasts 

```{r verifydiffyoytoyoy_fc}

rgdp_yoy_2015q3 <- window(rgdp_yoy, start=c(2015, 3), end=c(2015,3))

cv_diffyoy_fc_2015q4 <- window(fc_rgdp_raw_cv4, start=c(2015, 4), end=c(2015,4))
cv_diffyoy_fc_2016q1 <- window(fc_rgdp_raw_cv4, start=c(2016, 1), end=c(2016,1))

# first yoy forecast use yoy data and diff_yoy forecast
manual_fc_yoy_2015q4 <- as.numeric(rgdp_yoy_2015q3) + as.numeric(cv_diffyoy_fc_2015q4)

# second yoy forecast use previous yoy forecast and diff_yoy forecast
manual_fc_yoy_2016q1 <- as.numeric(manual_fc_yoy_2015q4) + as.numeric(cv_diffyoy_fc_2016q1)

# compare this manually computed values to those given by the function
# these should be equal
print(c(manual_fc_yoy_2015q4, fc_rgdp_yoy_cv4[1]))

# these should be equal
print(c(manual_fc_yoy_2016q1, fc_rgdp_yoy_cv4[2]))



```

So it works too for predictions as well.


Finally we can get to the yoy forecast errors for that fourth cv-round:

```{r}
# fe mean forecast errors
fe_rgdp_yoy_cv4 <- td_rgdp_yoy_cv4  - fc_rgdp_yoy_cv4 
fe_rgdp_yoy_cv4
```


The same must be applied to the other nine cv rounds, fortunately there is a helping function that does exactly that:

```{r transform_entire_cv_obj}

one_specification <- one_specification %>%
        rename(cv_obj_diff_yoy = cv_obj) %>%
        mutate(cv_obj_yoy = map(cv_obj_diff_yoy,
                                ~ transform_all_cv( .,
                                      current_form = "diff_yoy",
                                      target_level_ts =  rgdp_level,
                                      n_cv = n_cv)
              )
        )
  
# these are the errors we want to evaluate in the end
cv_errors_yoy <- pluck(one_specification, "cv_obj_yoy", 1, "cv_errors")
                                
```
Now our tibble contains the original predictions and prediction errors (cv_obj_diff_yoy) and the transformed ones (cv_obj_yoy).


### Accuracy measures from forecast errors

If we collect yhe third element of each of the ten series of forecast errors we can have a notion of how good or bad this particular model is at making 3-steps-ahead forecast.

```{r rmsemaeyoy_manual}

all_3stepa <- map_dbl(cv_errors_yoy, 3)
sq_3stepa <- all_3stepa^2
mse_3stepa <- mean(sq_3stepa)
rmse_3stepa <- sqrt(mse_3stepa)

print(rmse_3stepa)

mae_3stepa <- mean(abs(all_3stepa))
print(mae_3stepa)

```


the helper function all_rmse_from_cv_obj does it for all forecast horizons

```{r rmsemaeyoy}
one_specification <- one_specification %>%
    mutate(rmse_yoy_all_h = map(cv_obj_yoy, all_rmse_from_cv_obj),
           mae_yoy_all_h =  map(cv_obj_yoy, all_mae_from_cv_obj))

rmse_tibble <- one_specification$rmse_yoy_all_h[[1]]
names(rmse_tibble) <- paste0("rmse_", seq(1, length(rmse_tibble)))

mae_tibble <- one_specification$mae_yoy_all_h[[1]]
names(mae_tibble) <- paste0("mae_", seq(1, length(mae_tibble)))


print(rmse_tibble)
print(mae_tibble)

```

In a next document, in the context of multiple models, how to rank them according to these accuracy measures.


### Save objects for future reference

```{r savingobjects}

target_transformation  <- "diff_yoy"
country <- "Uruguay"
all_VAR_data_transformed <- all_data_for_VAR_estimation
all_data_raw <- data_ts
models_tbl <- one_specification

saveRDS(object = list(country_name = country, 
                      target_transformation = target_transformation, 
                      raw_data = all_data_raw,
                      transformed_data = all_VAR_data_transformed,
                      models_tbl = models_tbl), 
        file = "./data/examples/example_one_model_ury.rds")


# saveRDS(object = list(country_name = country, 
#                       target_transformation = target_transformation, 
#                       raw_data = all_data_raw,
#                       transformed_data = all_VAR_data_transformed),
#         file = "./data/examples/example_data_ury.rds")


```






