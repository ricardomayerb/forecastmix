---
title: 'Multiple models: a two-models speedy case'
author: "Ricardo Mayer"
date: "12/6/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r source-and-lib, message=FALSE, warning=FALSE}
source('./R/combinations_functions.R')
```

We will start with a ready to use data set with quarterly, stationary series than can be used in an ordinary VAR, and we will not explain how the data munging is done. In particular, we will *not* discuss the following important points:
    - how the data was obtained
    - how monthly data was converted to quaterly frequency
    - how monthly data was *extended* to complete its current final quarter
    - how (potentially) exogenous data was forecasted in order to make it available to produce conditional forecasts
    - how each series was transformed using seasonal and oridinary differeces to render them stationary
    
All those points are discussed in the data preparation document, see *here*

## VAR-ready data set
    
We will use the example dataset with domestic series from Uruguay and few external series. The rds file contains the country's name, the transformation applied to rgdp to render it stationary and two data sets: the original or raw data and one ready to used in VAR estimation,  containing only stationary versions of the original series. 


```{r loading_data}
data_object_ury <- readRDS("./data/examples/example_data_ury.rds")
print(names(data_object_ury))
country <- data_object_ury$country_name
target_transformation <- data_object_ury$target_transformation
raw_data <- data_object_ury$raw_data
var_data <- data_object_ury$transformed_data
print(target_transformation)

names_exogenous <- c("ip_us","ip_asia","ip_ue","ip_bra","act_eco_bra","emae_arg")

```

In this case, all VARs will use a  "diff-yoy" transformation of the real GDP series (i.e. first, take seasonal differences on the quarterly series and then ordinary differences on the result).    


## Counting specifications

The total number of potential specifications depends on a number of factors:
 - number of variable combinations. Which in turn depends on:
    - the total number of variables in the data set
    - the number of variables in the VAR (the "size" of the VAR: 2, 3, 4 ...)
    - the number of exogenous variables in the data set. This is because we generally choose leave out VARs where there is only one endogenous variable and all the rest are exogenous. That's more properly called an ARIMAX model. The default is to ignore such models when they show up, but it can be changed.
 - number of maximum lags to consider: e.g. 3, 4, 5 and 6
 - number of restricted version to consider: unrestricted, t = 1.65 and t = 2
 
 With two restricted version, plus the unrestricted one and four possible lag choices, we generate 12 specification per each variable combination we submit. A more modest inquiry may examine only unrestricted models for two lag choices, in which case ge only generate 2 specifications per tuple of variables. Say we have 500 combinations of variables to try out, that would tipically imply between 2x500 = 1000 and 12x500 = 6000 specifications to estimate, test and do cross-validation. 
 
A formula of the number of combinations, ignoring the distinction between endogenous and exogenpus variables, can be written as:

$$ncomb = \frac{(n - n_f)!}{(n - n_f - s - n_f)! ~ (s-n_f)!} = \frac{(n_a)!}{(n_a - s_a)! ~ (s_a)!}$$

Where $n$ is the total number of variables in the data set, $s$ is the number of distinct variables in the VAR (the "size") and $n_f$ is the number of *fixed variables*, i.e. those that need to be in any VAR (tipically $rgdp$ in our examples but it is *always* an endogenous variable and since we have at least one target variable it is at least equal to one). It can be more succintly expressed in the number of adjusted (for combinatorial purposes) numbers of variables to choose from,  $n_a := n -n_f$, and the adjusted  numbers of slots to fill, $s_a := s - n_f$. 

If we wanted to exclude those VAR with just one endogenous variables, then we can adjust $ncomb$ above by this quantity (notice that this case can only happen when we have only one endogenous variable acting as fixed variable):

$$ncomb_x = \frac{n_x!}{(n_x - (s-1))! ~ (s-1)!}$$
Where $n_x$ is the number of variables that we consider as exogenous. Notice that this number is zero in whenever there is less objects to choose from than the number of slots to be fill i.e when $n_x < s-1$.

Finally, we could define

$$ncomb^* = ncomb - ncomb_x$$

as the number of combinations, adjusted by ignoring VARs with only one endogenous variables. The table below shows, however that for the case of Uruguay (and it will be the case for the rest of our countries) it makes very little difference in the final number of variable combinations.
 

```{r countingcombinations}


ncombs <- map(2:7, ~ count_combn(var_size = .x, n_total = 31, n_exo = 6, n_fixed = 1))
ntable <- as_tibble(cbind(n = ncol(var_data), n_fixed = 1, size = 2:7, reduce(ncombs, rbind)))
print(ntable)

# ncombs41 <- map(2:7, ~ count_combn(var_size = .x, n_total = 41, n_exo = 6, n_fixed = 1))
# ntable41 <- as_tibble(cbind(n = ncol(var_data), n_fixed = 1, size = 2:7, reduce(ncombs41, rbind)))
# print(ntable41)
# 
# ncombs10 <- map(2:7, ~ count_combn(var_size = .x, n_total = 10, n_exo = 6, n_fixed = 1))
# ntable10 <- as_tibble(cbind(n = ncol(var_data), n_fixed = 1, size = 2:7, reduce(ncombs10, rbind)))
# print(ntable10)
```

 
Notice how rapidly the number of combinations increases with VAR size. Since $comb_x$ is relatively small here, for the next table we will consider $ncomb$ alone. Depending on the number of different lags and restrictions the final number of specification will tipically  be from two (no restrictions and two lag choices) to twelve times (two restrictions and four lag choices) the number of variable combinations. In the following table we consider the resulting number of specification under those two scenarios plus the number of estimations made taking into account ten rounds of cross validations in addition to the full sample original estimate. Since at least some models will fail the tests, only a fraction of them will qualify to be passed to cross-validation. The table below assume that between 20 to 90 percent of the models will pass. So the minimum number of VAR estimation to do happens in the no-restriction, two lag choices and 20% of qulifiying, whereas the maximum number of estimation would happens if 90% of the models pass the test and we are trying four lag choices and two restrictions.


```{r nspecifications}

nspectable <- ntable %>% dplyr::select(size, ncomb)

nspectable <- nspectable %>% 
  mutate(nVAR_2 = ncomb*2, nVAR_12 = ncomb*12, cv_min = nVAR_2*0.2*10, cv_max = nVAR_12*0.9*10)

print(nspectable)


```

Even with a modest size of 4, we can be dealing with 50,000 specification (and tests) and almost half a million estimations. Projecting how long those estimatiation would take, needs to consider that restricted estimations are slower than unrestrited ones and we will leave that for later. By this time we just want to notice that in order to use VAR of size 5 we either confine ourselves to unrestricted models and few lag choices or we need to find a strategy to select variables that lowers the number of possible combinations to be explored.

To expede wthing up in this document we will restrict oour attention to size-3 VARs, usig at first two variable combinations and at the end using all of them.



## Generate all specifications for a given size



```{r tuplesofvbls, cache=TRUE}

var_size <- 3 
all_variables <- colnames(var_data)
target_variable <- "rgdp"
non_target_fixed <- c("")


# 1742
tic()
specifications_size_3_all_u <- all_specifications(
  var_size = 3, all_variables = colnames(var_data),
  lag_choices = c(3,4,5), use_info_lags = TRUE,
  var_data = var_data, t_thresholds = 0, names_exogenous = names_exogenous)
toc()

# 1742
tic()
specifications_size_3_all_u_noinfolags <- all_specifications(
  var_size = 3, all_variables = colnames(var_data),
  lag_choices = c(3,4,5), use_info_lags = FALSE,
  var_data = var_data, t_thresholds = 0, names_exogenous = names_exogenous)
toc()

# 1742
tic()
specifications_size_3_all <- all_specifications(
  var_size = 3, all_variables = colnames(var_data),
  lag_choices = c(3,4,5), use_info_lags = TRUE,
  var_data = var_data, t_thresholds = c(1.65, 2), names_exogenous = names_exogenous)
toc()

# 1305
tic()
specifications_size_3_all_noinfolag <- all_specifications(
  var_size = 3, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = c(1.65, 2), names_exogenous = names_exogenous)
toc()



```





### Select a subset of specification for expediency in this document
```{r s3smmd}

specifications_size_3_17 <- specifications_size_3_all[1:17, ] 
specifications_size_3_170 <- specifications_size_3_all[1:170, ] 

specifications_size_3_17_u <- specifications_size_3_all_u[1:17, ] 
specifications_size_3_170_u <- specifications_size_3_all_u[1:170, ] 

```


## Fit all specifications and keep only acceptable ones

```{r  passing_models_size3small, cache=TRUE}
ftmt_size_3_17 <- fit_tests_models_table(specifications_size_3_17, var_data = var_data, names_exogenous = names_exogenous)
pm_size_3_17 <- ftmt_size_3_17[["passing_models"]]
print(head(pm_size_3_17))

ftmt_size_3_17_u <- fit_tests_models_table(specifications_size_3_17_u, var_data = var_data, names_exogenous = names_exogenous)
pm_size_3_17_u <- ftmt_size_3_17_u[["passing_models"]]
print(head(pm_size_3_17_u))

pm_3specs <- pm_size_3_17[c(1, 11, 21), ]


```


## Time Series Cross Validation on valid specifications


### Preparation: forecast exogenous values in advance

```{r extend_exo_cv,cache=TRUE}
exodata_fullsample <- var_data[,names_exogenous]
target_used_in_VAR <- var_data[, target_variable]
start_target_in_VAR <- start(na.omit(target_used_in_VAR))
end_target_in_VAR <- end(na.omit(target_used_in_VAR))
fc_horizon <- 8
n_cv <- 10

tic()
extension_of_exo <- extending_exogenous(exodata = exodata_fullsample,
                                        h = fc_horizon,
                                        endo_end = end_target_in_VAR)
toc()

tic()
cv_extension_of_exo <- extending_exogenous_for_cv(
  exodata = exodata_fullsample, h = fc_horizon, endo_end = end_target_in_VAR, 
  n_cv = n_cv, same_model_across_cv = FALSE)
toc()

```


### Cross Validation





```{r tscv_s3small, cache=TRUE}
training_length <- "per_cv_maxs" 
fit_column <- "fit"
target_transform <- target_transformation
target_level_ts <- na.omit(raw_data[, target_variable])

tic()
cv_size_3_17 <- cv_var_from_model_tbl(h = fc_horizon,
                             training_length = training_length, 
                             n_cv = n_cv,
                             models_tbl = pm_size_3_17, 
                             var_data = var_data, 
                             fit_column = "fit", 
                             target_transform = target_transform,
                             target_level_ts = target_level_ts, 
                             names_exogenous = names_exogenous, 
                             future_exo = extension_of_exo, 
                             extended_exo_mts = cv_extension_of_exo,
                             keep_varest_obj = FALSE
                             )
toc()

tic()
cv_size_3_17_with_varest <- cv_var_from_model_tbl(h = fc_horizon,
                             training_length = training_length, 
                             n_cv = n_cv,
                             models_tbl = pm_size_3_17, 
                             var_data = var_data, 
                             fit_column = "fit", 
                             target_transform = target_transform,
                             target_level_ts = target_level_ts, 
                             names_exogenous = names_exogenous, 
                             future_exo = extension_of_exo, 
                             extended_exo_mts = cv_extension_of_exo,
                             keep_varest_obj = TRUE
                             )
toc()


tic()
cv_3s <- cv_var_from_model_tbl(h = fc_horizon,
                             training_length = training_length, 
                             n_cv = n_cv,
                             models_tbl = pm_3specs, 
                             var_data = var_data, 
                             fit_column = "fit", 
                             target_transform = target_transform,
                             target_level_ts = target_level_ts, 
                             names_exogenous = names_exogenous, 
                             future_exo = extension_of_exo, 
                             extended_exo_mts = cv_extension_of_exo,
                             keep_varest_obj = FALSE
                             )
toc()

tic()
cv_3s_with_varest <- cv_var_from_model_tbl(h = fc_horizon,
                             training_length = training_length, 
                             n_cv = n_cv,
                             models_tbl = pm_3specs, 
                             var_data = var_data, 
                             fit_column = "fit", 
                             target_transform = target_transform,
                             target_level_ts = target_level_ts, 
                             names_exogenous = names_exogenous, 
                             future_exo = extension_of_exo, 
                             extended_exo_mts = cv_extension_of_exo,
                             keep_varest_obj = TRUE
                             )
toc()

# tic()
# cv_size_3_170 <- cv_var_from_model_tbl(h = fc_horizon,
#                              training_length = training_length, 
#                              n_cv = n_cv,
#                              models_tbl = pm_size_3_170, 
#                              var_data = var_data, 
#                              fit_column = "fit", 
#                              target_transform = target_transform,
#                              target_level_ts = target_level_ts,
#                              )
# toc() 

```



## Combine forecasts based on performance measure



The function forecast_var_from_model_tbl by default, it just take all specifications in given tibble and produce forecasts of the target variable, more precisely it creates a list-column containing mean forecasts of YoY growth of the target variable over the specified forecast horizon. Some options include to keep varest or forecast objects (not advised with very long tibbles) and more importantly it can restrict the sample of models by keeping only the best  models at each horizon and computing a (rmse-)weighted average mean forecast.

In this case we take our models from the cv-exercises, thus all models where already tested for stability, restrictions and white noise residuals and therefore we can spare those test here, which is the default behaviour.


```{r forecast_using_cv}

# forecast all 21 models
fcs_from_all <- forecast_var_from_model_tbl(
  cv_size_3_17,
  var_data, 
  fc_horizon, 
  target_transform = target_transform,
  target_level_ts = target_level_ts,
  names_exogenous = names_exogenous, 
  extended_exo_mts = extension_of_exo
)

fcs_from_all_ve <- forecast_var_from_model_tbl(
  cv_size_3_17_with_varest, 
  fit_column = "fit",
  var_data, 
  fc_horizon, 
  target_transform = target_transform,
  target_level_ts = target_level_ts,
  names_exogenous = names_exogenous, 
  extended_exo_mts = extension_of_exo
)

max_rank_h <- 4

fc_best_per_h <- forecast_var_from_model_tbl(
  cv_size_3_17,
  var_data, 
  fc_horizon, 
  target_transform = target_transform,
  target_level_ts = target_level_ts,
  names_exogenous = names_exogenous, 
  extended_exo_mts = extension_of_exo,
  do_rmse_average = TRUE,
  max_rank_h = max_rank_h, 
  filter_by_rank = TRUE) 

fc_rmse_weighted <- fc_best_per_h$fcs_wavg
models_tbl_max_rank <- fc_best_per_h$models_tbl
models_tbl_max_rank
snames_best <-  models_tbl_max_rank %>% 
  dplyr::select(short_name) %>% distinct()
snames_best

models_tbl_used_fc <- semi_join(cv_size_3_17, snames_best, by = "short_name")
models_tbl_not_used_fc <- anti_join(cv_size_3_17, snames_best, by = "short_name")


fc_best_per_h_ve <- forecast_var_from_model_tbl(
  cv_size_3_17_with_varest,
  fit_column = "fit",
  var_data, 
  fc_horizon, 
  target_transform = target_transform,
  target_level_ts = target_level_ts,
  names_exogenous = names_exogenous, 
  extended_exo_mts = extension_of_exo,
  do_rmse_average = TRUE,
  max_rank_h = max_rank_h, 
  filter_by_rank = TRUE) 

foo <- fc_best_per_h$models_tbl
foove <- fc_best_per_h_ve$models_tbl

fooved <- foove %>% distinct(short_name, .keep_all = TRUE)

food <- foo %>% distinct(short_name, .keep_all = TRUE)

```



```{r cv_of_ensemble}

variables_used <- fc_best_per_h$models_tbl %>% 
  dplyr::select(variables) %>% unlist() %>% unique()
var_data_best_fc <- var_data[, variables_used]

h_max <- fc_horizon
  
training_length <- "per_cv_maxs"
training_length <- "common_max"
training_length <- 28

if (training_length == "common_max") {
    total_obs <- nrow( na.omit(var_data[, variables_used]))
    training_length <- total_obs - h_max - (n_cv - 1)
    print(paste0("common_max = ", training_length))
}

train_test_dates <- make_test_dates_list(
  ts_data = na.omit(var_data[, variables_used]), 
  type = "tscv",
  n = n_cv, 
  h_max = fc_horizon, 
  training_length = training_length)

# tail(var_data_train)

cv_ensemble_fcs <- list_along(seq(1, n_cv))
cv_ensemble_test_data <- list_along(seq(1, n_cv))
cv_ensemble_errors <- list_along(seq(1, n_cv))

for (i in seq(1, n_cv)) {
  
  train_test_yq <- train_test_dates[["list_of_year_quarter"]]

  this_tra_s <- train_test_yq[[i]]$tra_s
  this_tra_e <- train_test_yq[[i]]$tra_e
  
  this_tes_s <- train_test_yq[[i]]$tes_s
  this_tes_e <- train_test_yq[[i]]$tes_e
  
  print(this_tra_e)
  print(this_tra_s)
  
  var_data_train <- window(var_data, start = this_tra_s, end = this_tra_e)
  
  # print(nrow(var_data_train))
  # print(head(var_data_train, n = 1))
  # print(tail(var_data_train, n = 1))
  
  this_fc_tbl <- forecast_var_from_model_tbl(
    models_tbl = cv_size_3_17, 
    fc_horizon = fc_horizon, 
    var_data = var_data_train,
    target_transform = target_transform,
    target_level_ts = target_level_ts,
    names_exogenous = names_exogenous, 
    extended_exo_mts = cv_extension_of_exo[[i]],
    do_rmse_average = TRUE,
    max_rank_h = 5,
    filter_by_rank = TRUE) 

  ensemble_fc_rmse_weighted <- this_fc_tbl$fcs_wavg
  
  print("ensemble_fc_rmse_weighted")
  print(ensemble_fc_rmse_weighted)
  
  test_target_yoy <- window(make_yoy_ts(target_level_ts), 
                           start = this_tes_s,
                           end = this_tes_e)
  ensemble_cv_error_yoy <- test_target_yoy - ensemble_fc_rmse_weighted
  
  cv_ensemble_fcs[[i]] <- ensemble_fc_rmse_weighted
  cv_ensemble_test_data[[i]] <- test_target_yoy
  cv_ensemble_errors[[i]] <- ensemble_cv_error_yoy
}

mat_cv_errors_ensemble <- matrix(
    reduce(cv_ensemble_errors, rbind), 
    nrow = n_cv)
rownames(mat_cv_errors_ensemble) <- NULL
ensemble_rmse <- sqrt(colMeans(mat_cv_errors_ensemble^2, na.rm = TRUE))

ensemble_rmse

boo <- this_fc_tbl[["models_tbl"]]


 # cv_of_VAR_ensemble(var_training_length = "common-max", 
#                    n_cv = n_cv,
#                    tbl_of_models_and_rmse = fc_best_5_per_h[["models_tbl"]],
#                    extended_x_data_ts = extension_of_exo,
#                    var_data = var_data,
#                    rgdp_level_ts = target_level_ts,
#                    max_rank_h = NULL,
#                    chosen_rmse_h = NULL,
#                    h_var = fc_horizon,
#                    ensemble_name = NULL)


```

this_fc_tbl

 
## Computing time and object size


### Specifications to test

For most of the comparison we will fix the the number of specifications at 100. Restricted estimation complicated this accounting a bit: with one threshold (e.g 1.65) it adds one unrestricted estimation per each restricted model, so it is effectively working with two specifications, adn with two thresholds (e.g. c(1.65, 2)) it estimated the unrestried model once and uses it for both restricted models, thus is closer to working with three specifications, not two or four. On top of that, restricted estimation are more expensive than unrestricted ones, thus computing times are not simply twice or thrice the computing time of the corresponding unrestricted models, and so it is a good idea time them explicitely.

Subseting of specifications are made by a random sample of the rows of all specifications. A more accurate estimate could be obtaining by taking several of this samples and average their resulting computing times. We expect some variation bc a given sample could result in larger or smaller fraction of models passing the tests and therefore selected for cross-validation, as we will see.


 
```{r spec_for_timings, cache=TRUE}

# unrestricted, all var_sizes
specs_s3_all_u_noinfolags <- all_specifications(
  var_size = 3, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = 0, silent = TRUE)

specs_s4_all_u_noinfolags <- all_specifications(
  var_size = 4, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = 0, silent = TRUE)

specs_s5_all_u_noinfolags <- all_specifications(
  var_size = 5, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = 0, silent = TRUE)

specs_s3_100_u_nil <- sample_n(specs_s3_all_u_noinfolags, 100)  
specs_s4_100_u_nil <- sample_n(specs_s4_all_u_noinfolags, 100)  
specs_s5_100_u_nil <- sample_n(specs_s5_all_u_noinfolags, 100)  


# restricted 1.65, all var_sizes
specs_s3_all_165_noinfolags <- all_specifications(
  var_size = 3, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = c(1.65), silent = TRUE)

specs_s4_all_165_noinfolags <- all_specifications(
  var_size = 4, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = c(1.65), silent = TRUE)

specs_s5_all_165_noinfolags <- all_specifications(
  var_size = 5, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = c(1.65), silent = TRUE)

specs_s3_100_165_nil <- sample_n(specs_s3_all_165_noinfolags, 100) 
specs_s4_100_165_nil <- sample_n(specs_s4_all_165_noinfolags, 100)  
specs_s5_100_165_nil <- sample_n(specs_s5_all_165_noinfolags, 100)  


# restricted 1.65 and 2, all var_sizes
specs_s3_all_165_2_noinfolags <- all_specifications(
  var_size = 3, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = c(1.65, 2), silent = TRUE)

specs_s4_all_165_2_noinfolags <- all_specifications(
  var_size = 4, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = c(1.65, 2), silent = TRUE)

specs_s5_all_165_2_noinfolags <- all_specifications(
  var_size = 5, all_variables = colnames(var_data), 
  lag_choices = c(3,4,5), use_info_lags = FALSE, 
  var_data = var_data, t_thresholds = c(1.65, 2), silent = TRUE)

specs_s3_100_165_2_nil <- sample_n(specs_s3_all_165_2_noinfolags, 100) 
specs_s4_100_165_2_nil <- sample_n(specs_s4_all_165_2_noinfolags, 100)  
specs_s5_100_165_2_nil <- sample_n(specs_s5_all_165_2_noinfolags, 100)  


print(nrow(specs_s3_all_u_noinfolags))
print(nrow(specs_s4_all_u_noinfolags))
print(nrow(specs_s5_all_u_noinfolags))
print(nrow(specs_s4_all_u_noinfolags)/nrow(specs_s3_all_u_noinfolags))
print(nrow(specs_s5_all_u_noinfolags)/nrow(specs_s3_all_u_noinfolags))


```



### Timing searches of unrestricted and restricted models


Time fit and cross validation on different types of models

```{r timing_functions, message=FALSE, warning=FALSE, include=FALSE, cache=TRUE}

this_target_ts <- na.omit(raw_data[,target_variable])


tic()
t_100_u <- time_fit_cv(var_data = var_data, target_level_ts = this_target_ts, 
                       reps = 1)
toc()

# a
tic()
t_100_r165 <- time_fit_cv(var_data = var_data, target_level_ts = this_target_ts,
                          reps = 1, t_thresholds = 1.65)
toc()


tic()
t_100_r165_r2 <- time_fit_cv(var_data = var_data, target_level_ts = this_target_ts,
                          reps = 1, t_thresholds = c(1.65, 2)) 
toc()
```

examining timings and counts

```{r averagetimings}
units_vec <- c(sec = 1, min = 1/60, hr = 1/3600) # seconds, minutes, hours

all_specs_timings_u <- t_100_u$ave_timing[5,] * c(nrow(specs_s3_all_u_noinfolags), nrow(specs_s4_all_u_noinfolags), nrow(specs_s5_all_u_noinfolags))/100
sum_all_specs_timings_u <- sum(all_specs_timings_u)

all_specs_timings_165 <- t_100_r165$ave_timing[5,] * c(nrow(specs_s3_all_u_noinfolags), nrow(specs_s4_all_u_noinfolags), nrow(specs_s5_all_u_noinfolags))/100
sum_all_specs_timings_165 <- sum(all_specs_timings_165)

all_specs_timings_165_2 <- t_100_r165_r2$ave_timing[5,] * c(nrow(specs_s3_all_u_noinfolags), nrow(specs_s4_all_u_noinfolags), nrow(specs_s5_all_u_noinfolags))/100
sum_all_specs_timings_165_2 <- sum(all_specs_timings_165_2)

print("Seconds per 100 unrestricted models of size 3, 4 and 5")
print(t_100_u$ave_timing)
print("Total time (fit+cv) of doing all specifications of size 3, 4 and 5")
print(sum_all_specs_timings_u*units_vec, digits = 2) 

print("Urestricted models and one threshold-restriction")
print(t_100_r165$ave_timing)
print("Total time (fit+cv) of doing all specifications of size 3, 4 and 5")
print(sum_all_specs_timings_165*units_vec, digits = 2)

print("Urestricted models and two threshold-restrictions")
print(t_100_r165_r2$ave_timing)
print("Total time (fit+cv) of doing all specifications of size 3, 4 and 5")
print(sum_all_specs_timings_165_2*units_vec, digits = 2) 



```

As expected, cross validation is takes about 10 times longer than the full sample fitting, even if cv does not test the models, but compute forecasting errors and does any necessary transformation to get forecasts and errors in yoy form. This approximate ratio is lower for specification with restrictions or larger number of coefficients, since there are additional tests to perform, restriction matrices to apply and computing the roots of characteristic polynomials get more costly, and also crucially, in models with t-restrictions a higher fraction of models are rejected before the cross validation, making it a bit cheaper.

Also, looking at the total (fit+cv) computing time por 100 estimation, increasing the size of the VAR (number of variables) is only a problem for models with restrictions: not only size 4 and 5 VARs have a grater number of possible specifications but also each them takes between 40% to a 100% more time than size-3 models.

However, the timings above are based on just one sample of 100 specifications for each type of models and we should expect some variations in the proportion of models that pass the tests and therefore the fraction of models going into cv will vary.

To reduce soem of that variablility we repeat the timings ten times, selecting different subsets of specifications in each repetition and averagings the results. Since that process would take a fair ammount of time, we did it elsewhere and now we will just load the results. Let's see how different these numbers are fro  the one-shot estimation from above.


```{r load_ten_reps_timings}
ten_reps_tur1 = readRDS("t100ur1.rds")
ten_reps_tu <- ten_reps_tur1$u
ten_reps_tr1 <- ten_reps_tur1$r1
ten_reps_tr2 <-  readRDS("t_100_r165_r2.rds")


all_specs_timings_u_ten_reps <- ten_reps_tu$ave_timing[5,] * c(nrow(specs_s3_all_u_noinfolags), nrow(specs_s4_all_u_noinfolags), nrow(specs_s5_all_u_noinfolags))/100
sum_all_specs_timings_u_ten_reps <- sum(all_specs_timings_u_ten_reps)

all_specs_timings_165_ten_reps <- ten_reps_tr1$ave_timing[5,] * c(nrow(specs_s3_all_u_noinfolags), nrow(specs_s4_all_u_noinfolags), nrow(specs_s5_all_u_noinfolags))/100
sum_all_specs_timings_165_ten_reps <- sum(all_specs_timings_165_ten_reps)

all_specs_timings_165_2_ten_reps <- ten_reps_tr2$ave_timing[5,] * c(nrow(specs_s3_all_u_noinfolags), nrow(specs_s4_all_u_noinfolags), nrow(specs_s5_all_u_noinfolags))/100
sum_all_specs_timings_165_2_ten_reps <- sum(all_specs_timings_165_2_ten_reps)

print("Seconds per 100 unrestricted models of size 3, 4 and 5 (avg. 10 of reps)")
print(ten_reps_tu$ave_timing)
print("Total time (fit+cv) of doing all specifications of size 3, 4 and 5")
print(sum_all_specs_timings_u_ten_reps*units_vec, digits = 2) 

print("Urestricted models and one threshold-restriction of size 3, 4 and 5 (avg. 10 of reps)")
print(ten_reps_tr1$ave_timing)
print("Total time (fit+cv) of doing all specifications of size 3, 4 and 5")
print(sum_all_specs_timings_165_ten_reps*units_vec, digits = 2)

print("Urestricted models and two threshold-restrictions")
print(ten_reps_tr2$ave_timing)
print("Total time (fit+cv) of doing all specifications of size 3, 4 and 5 (avg. 10 of reps)")
print(sum_all_specs_timings_165_2_ten_reps*units_vec, digits = 2) 

```

Numbers are largely similar: the estimated time of a full search looking at unrestricted models would take 5.2 hours, a full search trying models both unrestricted and one value of thresholds would take 33 hours and a full search that would include unrestricted and two different restricted version of models would take 61 hours, not so different from previous estimates of 4.9, 38 and 56 hours from before.

According to these lasts results, a single run of 100 specifications of  size 3 models of the three varieties would take $18.5 + 74.1 + 129 = 222$ seconds (329.3 for size 4, 380.8 for size for 5 and 932 seconds for all three sizes). So, timing models for all sizes takes more than 4 times what it takes to do size-3 models only. 

Reasonably accurate and fast estimations of hypotetical running times can be obtained if we 
- Time the execution times of size-3 models (using 100 specification for each type sampled randomly from the set of all specifications)
- Use a previously estimated ratio of executing times for sizes 3 and 4

The actual, absolut times, may vary from computer to computer but we assume that relative times should not. We could even assume that those ratios are a good guide for a differet data set and most of the differences will be captured when estimating and timing 100 size-3 specifications for each type of model. We call this, quick estimation of running times: you could use the time_size_3 function with one, two or three repetitions and it will be considerable faster than even one repetition all size models.


```{r quicktimesize3, cache=TRUE}

tic()
t3_rep1 <- time_size_3(var_data = var_data, target_level_ts = this_target_ts, reps = 1)
toc()

print(t3_rep1$total_time_u_all_specs/3600, digits = 2)
print(t3_rep1$total_time_r1_all_specs/3600, digits = 2)
print(t3_rep1$total_time_r2_all_specs/3600, digits = 2)


# tic()
# t3_rep2 <- time_size_3(var_data = var_data, target_level_ts = this_target_ts, reps = 2)
# toc()
# 
# 
# tic()
# t3_rep3 <- time_size_3(var_data = var_data, target_level_ts = this_target_ts, reps = 3)
# toc()

```

Thus, this suggests 5.2, 33 and 57 hours instead of  4.9, 38 and 56 hours given by the more thorough exercise. The numbers are reasonably close.



### Try smaller and bigger data set (15 and 41 variables)

```{r timing_ury_15}


names41 <- c(colnames(var_data), letters[1:10]) 
names15 <- colnames(var_data)[1:15]

time_fake_dataset <- function(num_vbls, ave_timing_u, ave_timing_r1, 
                              ave_timing_r2, lag_choices = c(1,3,5),
                              tosample = 100, searches_per_size = c(1, 1, 1),
                              n_non_target_fixed = 0) {
  
  
  
  if (length(num_vbls) > 1) {
    n_v_s3 <- num_vbls[1]
    n_v_s4 <- num_vbls[2]
    n_v_s5 <- num_vbls[3]
  } else {
    n_v_s3 <- num_vbls
    n_v_s4 <- num_vbls
    n_v_s5 <- num_vbls
  }
  
  letters702 <- c(letters, sapply(letters, function(x) paste0(x, letters)))
  
  names_s3 <- letters702[1:n_v_s3]
  names_s4 <- letters702[1:n_v_s4]
  names_s5 <- letters702[1:n_v_s5]
  
  
  if (length(n_non_target_fixed) > 1) {
    n_fix_s3 <- n_non_target_fixed[1]
    n_fix_s4 <- n_non_target_fixed[2]
    n_fix_s5 <- n_non_target_fixed[3]
  } else {
    n_fix_s3 <- n_non_target_fixed
    n_fix_s4 <- n_non_target_fixed
    n_fix_s5 <- n_non_target_fixed
  }
  
  if (n_fix_s3 > 0) {
    fix_s3 <- names_s3[2:(n_fix_s3+1)]
  } else {
    fix_s3 <- c("")
  }
  
  if (n_fix_s4 > 0) {
    fix_s4 <- names_s4[2:(n_fix_s4+1)]
  } else {
    fix_s4 <- c("")
  }
  
  if (n_fix_s5 > 0) {
    fix_s5 <- names_s4[2:(n_fix_s5+1)]
  } else {
    fix_s5 <- c("")
  }
  


  specs_s3 <- all_specifications(var_size = 3, all_variables = names_s3,
                                 lag_choices = lag_choices, 
                                 target_variable = names_s3[1], non_target_fixed = fix_s3)
  specs_s4 <- all_specifications(var_size = 4, all_variables = names_s4,
                                 lag_choices = lag_choices, 
                                 target_variable = names_s3[1], non_target_fixed = fix_s4)
  specs_s5 <- all_specifications(var_size = 5, all_variables = names_s5,
                                 lag_choices = lag_choices, 
                                 target_variable = names_s3[1], non_target_fixed = fix_s5)
  
  n_specs_full <- c(nrow(specs_s3), nrow(specs_s4), nrow(specs_s5))
  n_specs_searched <- n_specs_full * searches_per_size
  

  all_specs_timings <- ave_timing_u[5,]*n_specs_searched/tosample
  all_specs_timings_165 <- ave_timing_r1[5,]*n_specs_searched/tosample
  all_specs_timings_165_2 <- ave_timing_r2[5,]*n_specs_searched/tosample
  sum_all_specs_timings <- sum(all_specs_timings)
  sum_all_specs_timings_165 <- sum(all_specs_timings_165)
  sum_all_specs_timings_165_2 <- sum(all_specs_timings_165_2)
  
  specs_time_hrs <- c(sum_all_specs_timings, sum_all_specs_timings_165,
        sum_all_specs_timings_165_2)/3600
  
  return(list(n_specs_full = n_specs_full, 
              n_specs_searched = n_specs_searched, 
              specs_time_hrs = specs_time_hrs))
}


vec_n_vbls <- list(10, 11, 12, 13, 14, 15, 20, 25, 30, 35, 40)


t_full_search <- map(vec_n_vbls, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing)
    )

times_of_fs <- map_dfc(t_full_search, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_fs) <- c(paste0("n_", vec_n_vbls), "model_types")


t_2s4_0s5_search <- map(vec_n_vbls, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(1, 2, 0))
    )

times_of_2s4_0s5 <- map_dfc(t_2s4_0s5_search, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_2s4_0s5) <- c(paste0("n_", vec_n_vbls), "model_types")


t_2s4_0s5_search_two_lags <- map(vec_n_vbls, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(1, 2, 0), 
                                    lag_choices = c(1, 5))
    )

times_of_2s4_0s5_two_lags <- map_dfc(t_2s4_0s5_search_two_lags, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_2s4_0s5_two_lags) <- c(paste0("n_", vec_n_vbls), "model_types")

knitr::kable(times_of_fs, digits = 1)
knitr::kable(times_of_2s4_0s5, digits = 1)
knitr::kable(times_of_2s4_0s5_two_lags, digits = 1)

t_5s3_0s4_0s5 <- map(vec_n_vbls, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(5, 0, 0))
    )

times_of_5s3_0s4_0s5 <- map_dfc(t_5s3_0s4_0s5, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_5s3_0s4_0s5) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_5s3_0s4_0s5, digits = 1)


t_5s3_0s4_0s5_two_lags <- map(vec_n_vbls, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(5, 0, 0),
                                    lag_choices = c(1, 5))
    )

times_of_5s3_0s4_0s5_two_lags <- map_dfc(t_5s3_0s4_0s5_two_lags, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_5s3_0s4_0s5_two_lags) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_5s3_0s4_0s5_two_lags, digits = 1)




t_7s3_0s4_0s5 <- map(vec_n_vbls, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(7, 0, 0))
    )

times_of_7s3_0s4_0s5 <- map_dfc(t_7s3_0s4_0s5, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_7s3_0s4_0s5) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_7s3_0s4_0s5, digits = 1)


t_7s3_0s4_0s5_two_lags <- map(vec_n_vbls, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(7, 0, 0),
                                    lag_choices = c(1, 5))
    )

times_of_7s3_0s4_0s5_two_lags <- map_dfc(t_7s3_0s4_0s5_two_lags, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_7s3_0s4_0s5_two_lags) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_7s3_0s4_0s5_two_lags, digits = 1)


vec_n_vbls_25 <- list(10, 11, 12, 13, 14, 15, 20, 25, c(30, 25, 25), 
                   c(35, 25, 25), c(40, 25, 25))

vec_n_vbls_20 <- list(10, 11, 12, 13, 14, 15, 20, c(25, 20, 20), c(30, 20, 20), 
                   c(35, 20, 20), c(40, 20, 20))

t_3s4_0s5_search_n25 <- map(vec_n_vbls_25, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(1, 3, 0))
    )

times_of_3s4_0s5_n25 <- map_dfc(t_3s4_0s5_search_n25, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_3s4_0s5_n25) <- c(paste0("n_", vec_n_vbls), "model_types")

knitr::kable(times_of_3s4_0s5_n25 , digits = 1)

t_3s4_0s5_search_n20 <- map(vec_n_vbls_20, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(1, 3, 0))
    )

times_of_3s4_0s5_n20 <- map_dfc(t_3s4_0s5_search_n20, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_3s4_0s5_n20) <- c(paste0("n_", vec_n_vbls), "model_types")

knitr::kable(times_of_3s4_0s5_n20 , digits = 1)



t_5s3_0s4_0s5_n25 <- map(vec_n_vbls_25, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(5, 0, 0))
    )

times_of_5s3_0s4_0s5_n25 <- map_dfc(t_5s3_0s4_0s5_n25, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_5s3_0s4_0s5_n25) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_5s3_0s4_0s5_n25, digits = 1)


t_5s3_0s4_0s5_n20 <- map(vec_n_vbls_20, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(5, 0, 0))
    )

times_of_5s3_0s4_0s5_n20 <- map_dfc(t_5s3_0s4_0s5_n20, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_5s3_0s4_0s5_n20) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_5s3_0s4_0s5_n20, digits = 1)





t_7s3_0s4_0s5_n25 <- map(vec_n_vbls_25, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(7, 0, 0))
    )

times_of_7s3_0s4_0s5_n25 <- map_dfc(t_7s3_0s4_0s5_n25, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_7s3_0s4_0s5_n25) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_7s3_0s4_0s5_n25, digits = 1)








t_7s3_0s4_0s5_n20 <- map(vec_n_vbls_20, ~ time_fake_dataset(num_vbls = .x, 
                                    ave_timing_u = ten_reps_tu$ave_timing,
                                    ave_timing_r1 = ten_reps_tr1$ave_timing,
                                    ave_timing_r2 = ten_reps_tr2$ave_timing,
                                    searches_per_size = c(7, 0, 0))
    )

times_of_7s3_0s4_0s5_n20 <- map_dfc(t_7s3_0s4_0s5_n20, "specs_time_hrs") %>% 
  mutate(model_types = c("unrestriced", "unr_and_one_r", "unr_and_two_r"))
names(times_of_7s3_0s4_0s5_n20) <- c(paste0("n_", vec_n_vbls), "model_types")
knitr::kable(times_of_7s3_0s4_0s5_n20, digits = 1)

```



